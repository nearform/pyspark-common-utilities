{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9385f253-22cd-4438-974b-0bbf1add55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0545f893-0793-42cc-9a0d-8caca6fd6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utilities import remove_duplicates, fill_nulls,flatten_json,mask_dataframe\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d36a04f-a6f6-4468-a610-388fc3c68788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "def test_remove_duplicates():\n",
    "    df = spark.createDataFrame([\n",
    "        (\"Alice\", \"NY\"), (\"Alice\", \"NY\"), (\"Bob\", \"LA\")\n",
    "    ], [\"name\", \"city\"])\n",
    "    result = remove_duplicates(df)\n",
    "    assert result.count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b07b53-7cd5-41f2-9547-befe5db98d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "def test_fill_nulls():\n",
    "    df = spark.createDataFrame([\n",
    "        (None, \"NY\"), (\"Bob\", None)\n",
    "    ], [\"name\", \"city\"])\n",
    "    result = fill_nulls(df, {\"name\": \"NA\", \"city\": \"Unknown City\"})\n",
    "    rows = result.collect()\n",
    "    assert rows[0][\"name\"] == \"NA\"\n",
    "    assert rows[1][\"city\"] == \"Unknown City\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25ed35de-2f21-4190-9343-f30e6ce19e81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "def test_flatten_json_exploding_arrays():\n",
    "    data = [{\n",
    "        \"id\": 1,\n",
    "        \"name\": \"John\",\n",
    "        \"address\": {\"city\": \"NY\", \"zipcode\": 12345},\n",
    "        \"phones\": [\n",
    "            {\"type\": \"home\", \"number\": \"1234\"},\n",
    "            {\"type\": \"work\", \"number\": \"5678\"}\n",
    "        ]\n",
    "    }]\n",
    "    df = spark.read.json(spark.sparkContext.parallelize(data))\n",
    "\n",
    "    result=flatten_json(df,explode_arrays=True)\n",
    "    # Check flattened columns\n",
    "    expected_cols = {\"id\", \"name\", \"address_city\", \"address_zipcode\", \"phones_type\", \"phones_number\"}\n",
    "    assert set(result.columns) == expected_cols\n",
    "    # Check number of output rows count \n",
    "    assert result.count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b98b818-139d-4820-9cda-60dd92db968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "def test_flatten_json_no_exploding_arrays():\n",
    "    data = [{\n",
    "        \"id\": 1,\n",
    "        \"name\": \"John\",\n",
    "        \"address\": {\"city\": \"NY\", \"zipcode\": 12345},\n",
    "        \"phones\": [\n",
    "            {\"type\": \"home\", \"number\": \"1234\"},\n",
    "            {\"type\": \"work\", \"number\": \"5678\"}\n",
    "        ]\n",
    "    }]\n",
    "    df = spark.read.json(spark.sparkContext.parallelize(data))\n",
    "\n",
    "    result=flatten_json(df,explode_arrays=False)\n",
    "    # Check flattened columns\n",
    "    expected_cols = {\"id\", \"name\", \"address_city\", \"address_zipcode\", \"phones\"}\n",
    "    assert set(result.columns) == expected_cols\n",
    "    # Check number of output rows count \n",
    "    assert result.count() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb88d67-1ff0-41e7-bdb7-b4b02bf3e25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                        [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m__________________________________________ test_hash_mask __________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_hash_mask\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        masked = mask_dataframe(df, {\u001b[33m\"\u001b[39;49;00m\u001b[33memail\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mhash\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n",
      "        rows = [r.email \u001b[94mfor\u001b[39;49;00m r \u001b[95min\u001b[39;49;00m masked.select(\u001b[33m\"\u001b[39;49;00m\u001b[33memail\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).collect()]\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m val \u001b[95min\u001b[39;49;00m rows:\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mif\u001b[39;49;00m val \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
      "                \u001b[90m# sha2 returns binary type by default â†’ length 32 bytes\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[94massert\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(val, (\u001b[96mbytes\u001b[39;49;00m, \u001b[96mbytearray\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               AssertionError: assert False\u001b[0m\n",
      "\u001b[1m\u001b[31mE                +  where False = isinstance('ff8d9819fc0e12bf0d24892e45987e249a28dce836a85cad60e28eaaa8c6d976', (<class 'bytes'>, <class 'bytearray'>))\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/1l/msktd05x709gyzgp6zpjp86r0000gn/T/ipykernel_20546/4095272594.py\u001b[0m:34: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_ef77492432304a56a0be488b3bf83f2a.py::\u001b[1mtest_hash_mask\u001b[0m - AssertionError: assert False\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        [\n",
    "            (1, \"Alice\", \"alice@example.com\", \"9876543210\"),\n",
    "            (2, \"Bob\", \"bob@example.com\", \"9123456789\"),\n",
    "            (3, None, None, None),   # Null edge case\n",
    "        ],\n",
    "        [\"id\", \"name\", \"email\", \"phone\"]\n",
    "    )\n",
    "def test_full_mask():\n",
    "    masked = mask_dataframe(df, [\"name\"], default_mask=\"MASKED\")\n",
    "    rows = masked.select(\"name\").collect()\n",
    "    assert all(r.name == \"MASKED\" or r.name is None for r in rows)\n",
    "\n",
    "def test_partial_mask():\n",
    "    masked = mask_dataframe(df, {\"name\": \"partial\"})\n",
    "    rows = [r.name for r in masked.select(\"name\").collect()]\n",
    "\n",
    "    for original, masked_val in zip([\"Alice\", \"Bob\", None], rows):\n",
    "        if original is None:\n",
    "            assert masked_val is None\n",
    "        else:\n",
    "            assert masked_val.startswith(original[:2])  # first 2 preserved\n",
    "            assert set(masked_val[2:]) <= {\"*\"}        # rest are only '*'\n",
    "            assert len(masked_val) == len(original)    # length unchanged\n",
    "\n",
    "\n",
    "def test_hash_mask():\n",
    "    masked = mask_dataframe(df, {\"email\": \"hash\"})\n",
    "    rows = [r.email for r in masked.select(\"email\").collect()]\n",
    "\n",
    "    for val in rows:\n",
    "        if val is not None:\n",
    "            # sha2 returns 64-char hex string (not bytes!)\n",
    "            assert isinstance(val, str)\n",
    "            assert len(val) == 64\n",
    "            # ensure it only contains hex characters\n",
    "            int(val, 16)  # will raise ValueError if not valid hex\n",
    "\n",
    "\n",
    "def test_custom_expr_mask():\n",
    "    masked = mask_dataframe(df, {\"phone\": \"expr:concat('XXX', substr(phone, -4, 4))\"})\n",
    "    rows = [r.phone for r in masked.select(\"phone\").collect()]\n",
    "    assert rows[0] == \"XXX3210\"\n",
    "    assert rows[1] == \"XXX6789\"\n",
    "    assert rows[2] is None  # null case handled\n",
    "\n",
    "def test_skip_nonexistent_column():\n",
    "    # Should not raise error if column doesn't exist\n",
    "    masked = mask_dataframe(df, {\"nonexistent\": \"full\"})\n",
    "    assert \"nonexistent\" not in masked.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
