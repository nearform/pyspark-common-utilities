{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9385f253-22cd-4438-974b-0bbf1add55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0545f893-0793-42cc-9a0d-8caca6fd6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utilities import remove_duplicates, fill_nulls,flatten_json,mask_dataframe\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d36a04f-a6f6-4468-a610-388fc3c68788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 12) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "def test_remove_duplicates():\n",
    "    df = spark.createDataFrame([\n",
    "        (\"Alice\", \"NY\"), (\"Alice\", \"NY\"), (\"Bob\", \"LA\")\n",
    "    ], [\"name\", \"city\"])\n",
    "    result = remove_duplicates(df)\n",
    "    assert result.count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b07b53-7cd5-41f2-9547-befe5db98d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "def test_fill_nulls():\n",
    "    df = spark.createDataFrame([\n",
    "        (None, \"NY\"), (\"Bob\", None)\n",
    "    ], [\"name\", \"city\"])\n",
    "    result = fill_nulls(df, {\"name\": \"NA\", \"city\": \"Unknown City\"})\n",
    "    rows = result.collect()\n",
    "    assert rows[0][\"name\"] == \"NA\"\n",
    "    assert rows[1][\"city\"] == \"Unknown City\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ed35de-2f21-4190-9343-f30e6ce19e81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "def test_flatten_json_exploding_arrays():\n",
    "    data = [{\n",
    "        \"id\": 1,\n",
    "        \"name\": \"John\",\n",
    "        \"address\": {\"city\": \"NY\", \"zipcode\": 12345},\n",
    "        \"phones\": [\n",
    "            {\"type\": \"home\", \"number\": \"1234\"},\n",
    "            {\"type\": \"work\", \"number\": \"5678\"}\n",
    "        ]\n",
    "    }]\n",
    "    df = spark.read.json(spark.sparkContext.parallelize(data))\n",
    "\n",
    "    result=flatten_json(df,explode_arrays=True)\n",
    "    # Check flattened columns\n",
    "    expected_cols = {\"id\", \"name\", \"address_city\", \"address_zipcode\", \"phones_type\", \"phones_number\"}\n",
    "    assert set(result.columns) == expected_cols\n",
    "    # Check number of output rows count \n",
    "    assert result.count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b98b818-139d-4820-9cda-60dd92db968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "def test_flatten_json_no_exploding_arrays():\n",
    "    data = [{\n",
    "        \"id\": 1,\n",
    "        \"name\": \"John\",\n",
    "        \"address\": {\"city\": \"NY\", \"zipcode\": 12345},\n",
    "        \"phones\": [\n",
    "            {\"type\": \"home\", \"number\": \"1234\"},\n",
    "            {\"type\": \"work\", \"number\": \"5678\"}\n",
    "        ]\n",
    "    }]\n",
    "    df = spark.read.json(spark.sparkContext.parallelize(data))\n",
    "\n",
    "    result=flatten_json(df,explode_arrays=False)\n",
    "    # Check flattened columns\n",
    "    expected_cols = {\"id\", \"name\", \"address_city\", \"address_zipcode\", \"phones\"}\n",
    "    assert set(result.columns) == expected_cols\n",
    "    # Check number of output rows count \n",
    "    assert result.count() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb88d67-1ff0-41e7-bdb7-b4b02bf3e25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                        [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m________________________________________ test_partial_mask _________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_partial_mask\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        masked = mask_dataframe(df, {\u001b[33m\"\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mpartial\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n",
      "        rows = masked.select(\u001b[33m\"\u001b[39;49;00m\u001b[33mname\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).collect()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m rows[\u001b[94m0\u001b[39;49;00m].name.startswith(\u001b[33m\"\u001b[39;49;00m\u001b[33mAl\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m rows[\u001b[94m0\u001b[39;49;00m].name.endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m rows[\u001b[94m1\u001b[39;49;00m].name.startswith(\u001b[33m\"\u001b[39;49;00m\u001b[33mBo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[95mand\u001b[39;49;00m rows[\u001b[94m1\u001b[39;49;00m].name.endswith(\u001b[33m\"\u001b[39;49;00m\u001b[33m**\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert (True and False)\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where True = <built-in method startswith of str object at 0x1128e6eb0>('Bo')\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where <built-in method startswith of str object at 0x1128e6eb0> = 'Bo*'.startswith\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +      where 'Bo*' = Row(name='Bo*').name\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  and   False = <built-in method endswith of str object at 0x1128e6eb0>('**')\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +    where <built-in method endswith of str object at 0x1128e6eb0> = 'Bo*'.endswith\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +      where 'Bo*' = Row(name='Bo*').name\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/1l/msktd05x709gyzgp6zpjp86r0000gn/T/ipykernel_16698/2761416543.py\u001b[0m:18: AssertionError\n",
      "\u001b[31m\u001b[1m__________________________________________ test_hash_mask __________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_hash_mask\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        masked = mask_dataframe(df, {\u001b[33m\"\u001b[39;49;00m\u001b[33memail\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mhash\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m})\u001b[90m\u001b[39;49;00m\n",
      "        rows = masked.select(\u001b[33m\"\u001b[39;49;00m\u001b[33memail\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).collect()\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Ensure SHA2 hash length is 64 hex chars\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(r.email) == \u001b[94m64\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m r.email \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m r \u001b[95min\u001b[39;49;00m rows)\u001b[90m\u001b[39;49;00m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/1l/msktd05x709gyzgp6zpjp86r0000gn/T/ipykernel_16698/2761416543.py\u001b[0m:24: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      ".0 = <list_iterator object at 0x112819610>\n",
      "\n",
      ">   \u001b[0m\u001b[94massert\u001b[39;49;00m \u001b[96mall\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(r.email) == \u001b[94m64\u001b[39;49;00m \u001b[95mor\u001b[39;49;00m r.email \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94mfor\u001b[39;49;00m r \u001b[95min\u001b[39;49;00m rows)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   TypeError: object of type 'NoneType' has no len()\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/1l/msktd05x709gyzgp6zpjp86r0000gn/T/ipykernel_16698/2761416543.py\u001b[0m:24: TypeError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_a0a7a4c764434347a53bd6a9e4789499.py::\u001b[1mtest_partial_mask\u001b[0m - AssertionError: assert (True and False)\n",
      "\u001b[31mFAILED\u001b[0m t_a0a7a4c764434347a53bd6a9e4789499.py::\u001b[1mtest_hash_mask\u001b[0m - TypeError: object of type 'NoneType' has no len()\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        [\n",
    "            (1, \"Alice\", \"alice@example.com\", \"9876543210\"),\n",
    "            (2, \"Bob\", \"bob@example.com\", \"9123456789\"),\n",
    "            (3, None, None, None),   # Null edge case\n",
    "        ],\n",
    "        [\"id\", \"name\", \"email\", \"phone\"]\n",
    "    )\n",
    "def test_full_mask():\n",
    "    masked = mask_dataframe(df, [\"name\"], default_mask=\"MASKED\")\n",
    "    rows = masked.select(\"name\").collect()\n",
    "    assert all(r.name == \"MASKED\" or r.name is None for r in rows)\n",
    "\n",
    "def test_partial_mask():\n",
    "    masked = mask_dataframe(df, {\"name\": \"partial\"})\n",
    "    rows = [r.name for r in masked.select(\"name\").collect()]\n",
    "\n",
    "    for original, masked_val in zip([\"Alice\", \"Bob\", None], rows):\n",
    "        if original is None:\n",
    "            assert masked_val is None\n",
    "        else:\n",
    "            assert masked_val.startswith(original[:2])  # first 2 preserved\n",
    "            assert set(masked_val[2:]) <= {\"*\"}        # rest are only '*'\n",
    "            assert len(masked_val) == len(original)    # length unchanged\n",
    "\n",
    "\n",
    "def test_hash_mask():\n",
    "    masked = mask_dataframe(df, {\"email\": \"hash\"})\n",
    "    rows = masked.select(\"email\").collect()\n",
    "    # Ensure SHA2 hash length is 64 hex chars\n",
    "    assert all(len(r.email) == 64 or r.email is None for r in rows)\n",
    "\n",
    "def test_custom_expr_mask():\n",
    "    masked = mask_dataframe(df, {\"phone\": \"expr:concat('XXX', substr(phone, -4, 4))\"})\n",
    "    rows = [r.phone for r in masked.select(\"phone\").collect()]\n",
    "    assert rows[0] == \"XXX3210\"\n",
    "    assert rows[1] == \"XXX6789\"\n",
    "    assert rows[2] is None  # null case handled\n",
    "\n",
    "def test_skip_nonexistent_column():\n",
    "    # Should not raise error if column doesn't exist\n",
    "    masked = mask_dataframe(df, {\"nonexistent\": \"full\"})\n",
    "    assert \"nonexistent\" not in masked.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
